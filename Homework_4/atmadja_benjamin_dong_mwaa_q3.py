# -*- coding: utf-8 -*-
"""atmadja-benjamin-dong-mwaa-q3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zcQKZhqeX7wG2rPU-svGK5i2Vrb2rzBU
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
import boto3
from io import BytesIO
import matplotlib.pyplot as plt
import pendulum
import os

# DAG setup
local_tz = pendulum.timezone("America/Chicago")

default_args = {
    'owner': 'atmadja-benjamin-dong',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

S3_BUCKET = "atmadja-benjamin-dong-mwaa"
DATA_DIR = "weather_data"
OUTPUT_DIR = "output_graphs"

FIELDS_TO_PLOT = {
    "temperature": "Temperature (Â°C)",
    "visibility": "Visibility (m)",
    "relativeHumidity": "Relative Humidity (%)"
}

def create_and_upload_graphs(execution_date, **kwargs):
    s3 = boto3.client("s3")

    # Define date prefix based on execution date (Airflow passes it in UTC)
    run_date = execution_date.astimezone(local_tz).strftime('%Y%m%d')

    # List and download matching files
    all_files = s3.list_objects_v2(Bucket=S3_BUCKET, Prefix=DATA_DIR + "/").get("Contents", [])
    daily_files = [obj["Key"] for obj in all_files if run_date in obj["Key"] and obj["Key"].endswith(".csv")]

    if not daily_files:
        print("No weather data files found for", run_date)
        return

    # Read and combine all files
    dfs = []
    for key in daily_files:
        obj = s3.get_object(Bucket=S3_BUCKET, Key=key)
        df = pd.read_csv(obj["Body"])
        dfs.append(df)

    df_all = pd.concat(dfs, ignore_index=True)
    df_all["timestamp"] = pd.to_datetime(df_all["timestamp"])

    # Plot each field
    for field, label in FIELDS_TO_PLOT.items():
        plt.figure(figsize=(10, 6))
        for station in df_all["station"].unique():
            df_station = df_all[df_all["station"] == station]
            plt.plot(df_station["timestamp"], df_station[field], label=station)

        plt.title(f"{label} on {run_date}")
        plt.xlabel("Time")
        plt.ylabel(label)
        plt.legend()
        plt.grid(True)

        # Save to buffer
        img_buffer = BytesIO()
        plt.tight_layout()
        plt.savefig(img_buffer, format='png')
        img_buffer.seek(0)

        # Upload to S3
        filename = f"{OUTPUT_DIR}/{field}_{run_date}.png"
        s3.put_object(Bucket=S3_BUCKET, Key=filename, Body=img_buffer, ContentType='image/png')

        plt.close()

with DAG(
    'daily_weather_dashboard',
    default_args=default_args,
    description='Create daily weather graphs from S3 data',
    schedule_interval='@daily',
    start_date=datetime(2025, 6, 2, tzinfo=local_tz),
    catchup=False,
    tags=['weather', 'visualization', 'dashboard'],
) as dag:

    generate_dashboard = PythonOperator(
        task_id='create_weather_dashboard',
        python_callable=create_and_upload_graphs,
        provide_context=True,
    )