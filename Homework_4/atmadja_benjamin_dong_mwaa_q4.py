# -*- coding: utf-8 -*-
"""atmadja-benjamin-dong-mwaa-q4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_cPKEffMg5syoOW8P2k7ALuQEG3XwqVC
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import boto3
import pandas as pd
from io import StringIO
import pendulum
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Constants
S3_BUCKET = "atmadja-benjamin-dong-mwaa"
WEATHER_DATA_PREFIX = "weather_data/"
PREDICTION_OUTPUT_DIR = "predictions"

local_tz = pendulum.timezone("America/Chicago")

default_args = {
    'owner': 'atmadja-benjamin-dong',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def train_and_predict_model(**kwargs):
    s3 = boto3.client("s3")
    objects = s3.list_objects_v2(Bucket=S3_BUCKET, Prefix=WEATHER_DATA_PREFIX).get("Contents", [])

    if not objects:
        print("No weather data files available.")
        return

    dfs = []
    for obj in objects:
        if obj["Key"].endswith(".csv"):
            body = s3.get_object(Bucket=S3_BUCKET, Key=obj["Key"])["Body"]
            df = pd.read_csv(body)
            dfs.append(df)

    df_all = pd.concat(dfs, ignore_index=True)
    df_all["timestamp"] = pd.to_datetime(df_all["timestamp"])
    df_all = df_all.sort_values("timestamp")

    # Filter only rows with non-null temperature, station, timestamp
    df_all = df_all[df_all["temperature"].notna()]
    df_all = df_all.dropna(subset=["station", "timestamp"])

    # Fill NaNs in precipitation and heatIndex
    df_all["precipitationLastHour"] = df_all["precipitationLastHour"].fillna(0)
    df_all["heatIndex"] = df_all["heatIndex"].fillna(0)

    # Feature engineering
    df_all["minutes_since_start"] = (
        (df_all["timestamp"] - df_all["timestamp"].min()).dt.total_seconds() / 60
    )

    # Model training
    feature_cols = [
        "minutes_since_start", "station", "dewpoint", "windSpeed",
        "barometricPressure", "visibility", "precipitationLastHour",
        "relativeHumidity", "heatIndex"
    ]
    df_model = df_all.dropna(subset=feature_cols + ["temperature"])

    if df_model.empty:
        print("No valid rows to train the model.")
        return

    X = df_model[feature_cols]
    y = df_model["temperature"]

    categorical_features = ["station"]
    numeric_features = [col for col in feature_cols if col != "station"]

    preprocessor = ColumnTransformer(
        transformers=[
            ('cat', OneHotEncoder(handle_unknown="ignore"), categorical_features),
            ('num', 'passthrough', numeric_features)
        ]
    )

    model = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('regressor', LinearRegression())
    ])

    model.fit(X, y)

    # Prediction
    last_time = df_all["timestamp"].max()
    future_times = [last_time + timedelta(minutes=30 * i) for i in range(1, 17)]

    prediction_rows = []
    for station in df_all["station"].unique():
        latest_row = df_all[df_all["station"] == station].iloc[-1]

        latest_row["precipitationLastHour"] = latest_row.get("precipitationLastHour", 0) or 0
        latest_row["heatIndex"] = latest_row.get("heatIndex", 0) or 0

        for ft in future_times:
            row = {
                "station": station,
                "minutes_since_start": (ft - df_all["timestamp"].min()).total_seconds() / 60,
                "dewpoint": latest_row.get("dewpoint"),
                "windSpeed": latest_row.get("windSpeed"),
                "barometricPressure": latest_row.get("barometricPressure"),
                "visibility": latest_row.get("visibility"),
                "precipitationLastHour": latest_row["precipitationLastHour"],
                "relativeHumidity": latest_row.get("relativeHumidity"),
                "heatIndex": latest_row["heatIndex"],
            }
            prediction_rows.append(row)

    df_predict = pd.DataFrame(prediction_rows)
    df_predict["predictedTemperature"] = model.predict(df_predict)
    df_predict["predictionTime"] = [ft.isoformat() for ft in future_times * len(df_all["station"].unique())]

    # Upload result to S3
    timestamp_str = datetime.now(local_tz).strftime("%Y%m%dT%H%M%S")
    output_key = f"{PREDICTION_OUTPUT_DIR}/predictions_{timestamp_str}.csv"

    buffer = StringIO()
    df_predict.to_csv(buffer, index=False)
    s3.put_object(Bucket=S3_BUCKET, Key=output_key, Body=buffer.getvalue())

with DAG(
    'temperature_forecasting_model',
    default_args=default_args,
    description='Train a linear regression model after 20h and 40h of data',
    schedule_interval=None,
    start_date=datetime(2025, 6, 2, tzinfo=local_tz),
    catchup=False,
    tags=['weather', 'forecasting', 'ml'],
) as dag:

    train_and_predict = PythonOperator(
        task_id='train_predict_temperature',
        python_callable=train_and_predict_model,
        provide_context=True,
    )